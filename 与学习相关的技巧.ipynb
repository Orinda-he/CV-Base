{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fae9ffa",
   "metadata": {},
   "source": [
    "涉及寻找最优权重参数的最优化方法、权重参数的初始值，超参数的设定方法等，\n",
    "为了应对过拟合，学习权值衰减，Dropout等正则化方法，并进行实现，\n",
    "学习Batch Normalization 方法，通过这些方法可以高效的进行神经网络的学习，提高识别精度。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e50466",
   "metadata": {},
   "source": [
    "1、参数的更新\n",
    "2、权重的1初始值\n",
    "3、Batch Normalization\n",
    "4、正则化\n",
    "5、超参数的验证\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfdfd334",
   "metadata": {},
   "source": [
    "参数的更新\n",
    "\n",
    "神经网络的学习目的是找到使损失函数的值尽可能小的参数，这是寻找最优参数的问题，解决这个问题的过程称为最优化。\n",
    "\n",
    "随机梯度下降法（stochastic gradient descent），简称SGD\n",
    "    \n",
    "$$\n",
    "W = W - \\eta \\frac{\\partial L}{\\partial W}\n",
    "$$\n",
    "式中，$W$是权重参数，$\\eta$是学习率，$\\frac{\\partial L}{\\partial W}$是损失函数关于权重参数的导数。\n",
    "SGD是朝着梯度方向只前进一定距离的简单方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a852930a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr              #学习率\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        for key in params.keys():\n",
    "            params[key] -= self.lr * grads[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00896a4d",
   "metadata": {},
   "source": [
    "update（params,grads） 方法在SGD中被反复调用，参数params 和 grads 是字典型变量，按params['W1']、grads['W1']的形式，分别保存了权重参数和它们的梯度。使用SGD这个类，可以像下面这样更新权重参数。\n",
    "```\n",
    "# 假设神经网络有2层\n",
    "network = TwoLayerNet(...)\n",
    "optimizer = SGD()           #参数的更新由optimizer负责完成，需要做的只是将参数和梯度的信息传给optimizer\n",
    "\n",
    "for i in range(10000):\n",
    "    ...\n",
    "    # 进行mini-batch学习\n",
    "    x_batch, t_batch = get_mini_batch(...)  # mini-batch\n",
    "    grads = network.gradient(x_batch, t_batch)\n",
    "    optimizer.update(network.params, grads)\n",
    "    ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7711c96c",
   "metadata": {},
   "source": [
    "通过单独实现进行最优化的类，功能的模块化变得简单，如果要实现另外一个最优化Momentum，它同样会实现成拥有update(params, grads)这个共同方法的形式。这样一来，只需要将optimizer = SGD()这一语句换成optimizer = Momentum()，就可以从SGD切换为Momentum。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3952c1",
   "metadata": {},
   "source": [
    "SGD的缺点：\n",
    "- 学习率lr是固定值，不能自动调整。\n",
    "- 不能保证到达最小值。\n",
    "- 可能被困在鞍点（saddle point）。\n",
    "\n",
    "为了改正SGD 将介绍Momentum、AdaGrad、Adam 这3种方法来取代SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5002a580",
   "metadata": {},
   "source": [
    "Momentum 动量法\n",
    "公式\n",
    "$$\n",
    "v = \\alpha v - \\eta \\frac{\\partial L}{\\partial W} \\\\\n",
    "W = W + v\n",
    "$$\n",
    "其中，$v$是速度，$\\alpha$是动量系数，$\\eta$是学习率，$\\frac{\\partial L}{\\partial W}$是损失函数关于权重$W$的梯度。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a53249",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Momentum:\n",
    "    def __init__(self, lr=0.01, momentum=0.9):\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.v = None\n",
    "\n",
    "    def update(self, params, grads):\n",
    "        if self.v is None:\n",
    "            self.v = {}\n",
    "            for key, val in params.items():\n",
    "                self.v[key] = np.zeros_like(val)\n",
    "        for key in params.keys():\n",
    "            self.v[key] = self.momentum*self.v[key] - self.lr*grads[key]\n",
    "            params[key] += self.v[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2524ad80",
   "metadata": {},
   "source": [
    "实例变量v会保存物体的速度。初始化时，v中什么都不保存，但当第一次调用update()时，v会以字典型变量的形式保存与参数结构相同的数据。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f032c2a",
   "metadata": {},
   "source": [
    "Adagrad \n",
    "\n",
    "Adagrad 是一个针对学习率进行自适应调整的方法。学习率过小，会导致学习花费过多时间，学习率过大，会导致学习发散而不能正确进行。\n",
    "\n",
    "在关于学习率的有效技巧中，有一种被称为学习率衰减（learning rate decay）的方法。学习率衰减是指随着学习的进行，学习率逐渐减小的方法。这样做的目的是为了在学习开始时，使学习率较大，从而快速学习；而在学习结束时，使学习率较小，从而使学习更加稳定,逐渐减小学习率的想法，相当于将全体参数的学习率值一起降低而AdaGrad进一步发展了这个想法，针对“一个一个”的参数，赋予其“定制”的值。\n",
    "\n",
    "AdaGrad会为参数的每个元素适当地调整学习率，与此同时进行学习（AdaGrad的Ada来自英文单词Adaptive，即“适当的”的意思）。下面，让我们用数学式表示AdaGrad的更新方法。\n",
    "\n",
    "公式\n",
    "$$\n",
    "h = h + \\frac{\\partial L}{\\partial W} \\odot \\frac{\\partial L}{\\partial W} \\\\\n",
    "W = W - \\eta \\frac{1}{\\sqrt{h}} \\odot \\frac{\\partial L}{\\partial W}\n",
    "$$\n",
    "其中，$h$是累加的梯度的平方和，$\\eta$是学习率，$\\odot$是按元素相乘的运算符。\n",
    "变量h保存了以前所有梯度值的平方和，参数的元素中变动较大（被大幅更新）的元素的学习率将变小。也就是说，可以按参数的元素进行学习率衰减，使变动大的参数的学习率逐渐减小。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8f5761",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adagrad:\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "        self.h = None\n",
    "    def update(self, params, grads):\n",
    "        if self.h is None:\n",
    "            self.h = {}\n",
    "            for key, val in params.items():\n",
    "                self.h[key] = np.zeros_like(val)\n",
    "        for key in params.keys():\n",
    "            self.h[key] += grads[key] * grads[key]\n",
    "            params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + 1e-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ac7b60",
   "metadata": {},
   "source": [
    "这里需要注意的是，最后一行加上了微小值1e-7。这是为了防止当self.h[key]中有0时，将0用作除数的情况。在很多深度学习的框架中，这个微小值也可以设定为参数，但这里我们用的是1e-7这个固定值。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b54e96",
   "metadata": {},
   "source": [
    "Adam\n",
    "公式\n",
    "$$\n",
    "m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) \\triangledown \\theta_t\n",
    "$$\n",
    "$$\n",
    "v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) \\triangledown \\theta_t^2\n",
    "$$\n",
    "$$\n",
    "\\theta_t = \\theta_{t-1} - \\frac{\\alpha \\sqrt{1 - \\beta_2^t}}{1 - \\beta_1^t} \\frac{m_t}{\\sqrt{v_t} + \\epsilon}\n",
    "$$\n",
    "\n",
    "\n",
    "Momentum参照小球在碗中滚动的物理规则进行移动，AdaGrad为参数的每个元素适当地调整更新步伐。如果将这两个方法融合在一起会怎么样?\n",
    "它的理论有些复杂，直观地讲，就是融合了Momentum和AdaGrad的方法。通过组合前面两个方法的优点，有望实现参数空间的高效搜索。此外，进行超参数的“偏置校正”也是Adam的特征。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1702293f",
   "metadata": {},
   "source": [
    "Adam会设置3个超参数。一个是学习率（论文中以α出现），另外两个是一次momentum系数β1和二次momentum系数β2。根据论文，标准的设定值是β1为0.9，β2为0.999。设置了这些值后，大多数情况下都能顺利运行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166d981a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adam:\n",
    "    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999):\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.iter = 0\n",
    "        self.m = None\n",
    "        self.v = None\n",
    "    def update(self, params, grads):\n",
    "        if self.m is None:\n",
    "            self.m, self.v = {}, {}\n",
    "            for key, val in params.items():\n",
    "                self.m[key] = np.zeros_like(val)\n",
    "                self.v[key] = np.zeros_like(val)\n",
    "        self.iter += 1\n",
    "        lr_t = self.lr * np.sqrt(1.0 - self.beta2**self.iter) / (1.0 - self.beta1**self.iter)\n",
    "        for key in params.keys():\n",
    "            self.m[key] += (1 - self.beta1) * (grads[key] - self.m[key])\n",
    "            self.v[key] += (1 - self.beta2) * (grads[key]**2 - self.v[key])\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
